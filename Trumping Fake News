import numpy as np
import pandas as pd #to read the csv files
import matplotlib.pyplot as plt
import seaborn as sns #Seaborn helps you explore and understand your data. Its plotting functions operate on dataframes and arrays containing whole datasets and
#internally perform the necessary semantic mapping and statistical aggregation to produce informative plots
import itertools

from sklearn.model_selection import train_test_split #a function in Sklearn model selection for splitting data arrays into two subsets: for training data and for testing data
#from sklearn.feature_extraction.text import TfidfVectorizer #vectorizes text as an array
#from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics

from sklearn.utils import shuffle #Shuffles the data
import string #we use punctuations
import nltk
from nltk.corpus import stopwords #-> where wordnet is
from nltk import tokenize

from matplotlib.figure import Figure 
from matplotlib.backends.backend_tkagg import (FigureCanvasTkAgg, NavigationToolbar2Tk)
import tkinter as tk


#tkinter window
root = tk.Tk()
root.title("Home Screen")
root.state("zoomed")

#Background image
background_image = tk.PhotoImage(file="Background3.png")
bgimage_label = tk.Label(root, image=background_image)
bgimage_label.place(relwidth=1, relheight=1)

#Title frame
title_frame = tk.Frame(root, bg='#FAEBD7', bd=5)
title_frame.place(relx=0.5, rely=0.1, relwidth=0.75, relheight=0.2, anchor='n')

#Body frame
lower_frame = tk.Frame(root, bg='#FAEBD7', bd=10)
lower_frame.place(relx=0.5, rely=0.5, relwidth=0.75, relheight=0.2, anchor='n')



def _main_():
    
    #To clear all widgets from the screen and command to go back
    def forget_grid():
        title_grid = title_frame.grid_slaves()
        lower_grid = lower_frame.grid_slaves()
        for i in title_grid:
            i.grid_forget()
        for i in lower_grid:
            i.grid_forget()

    def forget_pack():  
        title_pack = title_frame.pack_slaves()
        lower_pack = lower_frame.pack_slaves()
        for i in title_pack:
            i.pack_forget()
        for i in lower_pack:
            i.lower_forget()

    def forget_root():
        root_pack = root.pack_slaves()
        for i in root.pack_slaves():
            i.pack_forget()
        root_grid = root.grid_slaves()
        for i in root.grid_slaves():
            i.grid_forget()
        
    
    #Reading from the data set:-
    fake = pd.read_csv("Fake.csv", low_memory=False)
    true = pd.read_csv("True.csv", low_memory=False)
    #Flagging the data as real or fake
    fake['target'] = 'fake'
    true['target'] = 'true'
    #Concatenate the real and fake news and shuffling it
    data = pd.concat([fake, true]).reset_index(drop = True) #reset_index makes it unique
    data = shuffle(data)
    data = data.reset_index(drop=True)


    #Now we remove the parts we don't require drop:- Remove rows or columns by specifying label names and corresponding axis, or by specifying directly index or column names
    #Since axis=1, drops from columns
    data.drop(["date"],axis=1,inplace=True) #date
    data.drop(["title"],axis=1,inplace=True) #title
    data['text'] = data['text'].apply(lambda x: x.lower())
    #data['text'] = list(map(lambda x: x.lower(), data['text']))

    #Removing punctuations and stop words
    def punctuation_removal(text):
        all_list = [char for char in text if char not in string.punctuation]
        clean_str = ''.join(all_list)
        return clean_str
    data['text'] = data['text'].apply(punctuation_removal)

    #nltk.download('stopwords')
    stop = stopwords.words('english') #words we can sacrifice w/o changing sentence meaning
    data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

    


    def Plot2():
        forget_root()
        forget_grid()
        forget_pack()
        
        #Plotting a graph on the no. of fake and real articles
        group_no = data.groupby(['target'])['text'].count()
        figure = Figure(figsize=(4, 3), dpi=100)
        ax = figure.add_subplot(111)
        chart_type = FigureCanvasTkAgg(figure, root)
        chart_type.get_tk_widget().pack()
        group_no.plot(kind='bar', legend=True, ax=ax)
        

    def Plot1():
        #Plotting graph of no. of articles per subject
        grouped_data = data.groupby(['subject'])['text'].count()
        #print(grouped_data)
        #data.groupby(['subject'])['text'].count().plot(kind="bar")
        #plt.title("No. of articles per news subject")
        #plt.show()

        figure = Figure(figsize=(4, 3), dpi=100)
        ax = figure.add_subplot(111)
        chart_type = FigureCanvasTkAgg(figure, root)
        chart_type.get_tk_widget().pack()
        #df = df[['First Column','Second Column']].groupby('First Column').sum()
        grouped_data.plot(kind='bar', legend=True, ax=ax)

        next_button= tk.Button(master = lower_frame, command = Plot2, height = 2, width = 50, text = "Next Graph")
        next_button.pack()

    plot1_button= tk.Button(master = lower_frame, command = Plot1, height = 2, width = 50, text = "Graph 1")
    plot1_button.pack()

        
    

'''
    #Frequently occuring words
    token_space = tokenize.WhitespaceTokenizer()
    def counter(text, column_text):
        all_words = ' '.join([text for text in text[column_text]])
        token_phrase = token_space.tokenize(all_words)
        frequency = nltk.FreqDist(token_phrase) #A dictionary that gives frequency of words
        df_frequency = pd.DataFrame({"Word": list(frequency.keys()), "Frequency": list(frequency.values())}) #DataFrame: A 2d panda array -> How a csv file is read
        df_frequency = df_frequency.nlargest(columns = "Frequency", n = 20) #The largest frequency of the words (first 20) 
        plt.figure(figsize=(12,8))
        ax = sns.barplot(data = df_frequency, x = "Word", y = "Frequency", color = 'blue') #Seaborn is used
        ax.set(ylabel = "Count")
        plt.xticks(rotation='vertical')
        plt.show()

    counter(data[data["target"] == "fake"], "text")
    counter(data[data["target"] == "true"], "text")


    def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):
        plt.imshow(cm, interpolation='nearest', cmap=cmap)
        plt.title(title)
        plt.colorbar()
        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes, rotation=45)
        plt.yticks(tick_marks, classes)
        if normalize:
            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            print("Normalized confusion matrix")
        else:
            print('Confusion matrix, without normalization')
            thresh = cm.max() / 2.
            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
                plt.text(j, i, cm[i, j], horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")

            plt.tight_layout()
            plt.ylabel('True label')
            plt.xlabel('Predicted label')
            plt.show()

    X_train, X_test, y_train, y_test = train_test_split(data['text'], data.target, test_size=0.2, random_state=42)


    #Logistics Regression
    # Vectorizing and applying TF-IDF
    from sklearn.linear_model import LogisticRegression
    pipe = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('model', LogisticRegression())])
    # Fitting the model
    model = pipe.fit(X_train, y_train)
    # Accuracy
    prediction = model.predict(X_test)
    print("accuracy: {}%".format(round(accuracy_score(y_test, prediction)*100,2)))

    cm = metrics.confusion_matrix(y_test, prediction)
    plot_confusion_matrix(cm, classes=['Fake', 'Real'])


    #Vectorizing and applying TF-IDF
    pipe = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()), ('model', DecisionTreeClassifier(criterion= 'entropy', max_depth = 20, splitter='best', random_state=42))])
    #Fitting the model
    model = pipe.fit(X_train, y_train)
    #Accuracy
    prediction = model.predict(X_test)
    print("accuracy: {}%".format(round(accuracy_score(y_test, prediction)*100,2)))

    cm = metrics.confusion_matrix(y_test, prediction)
    plot_confusion_matrix(cm, classes=['Fake', 'Real'])
    plt.show()


    pipe = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('model', RandomForestClassifier(n_estimators=50, criterion="entropy"))])
    model = pipe.fit(X_train, y_train)
    prediction = model.predict(X_test)
    print("accuracy: {}%".format(round(accuracy_score(y_test, prediction)*100,2)))

    cm = metrics.confusion_matrix(y_test, prediction)
    plot_confusion_matrix(cm, classes=['Fake', 'Real'])
    plt.show()
'''
    
_main_()
root.mainloop()

